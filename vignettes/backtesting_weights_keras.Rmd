---
title: "Backtesting Portfolio Weight Optimization with InvestigatoR"
author: "Dr. Sebastian StÃ¶ckl"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Backtesting Portfolio Weight Optimization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
# Set global options for knitr
knitr::opts_chunk$set(
  echo = TRUE,        # Display code chunks
  message = FALSE,    # Suppress messages
  warning = FALSE,    # Suppress warnings
  cache = TRUE        # Enable caching for computationally heavy chunks
)
```

# Introduction

Modern portfolio optimization requires balancing risk-adjusted returns with real-world constraints, such as transaction costs, leverage limits, and benchmark adherence. Traditional optimization techniques often struggle to incorporate these constraints effectively, leaving room for innovative approaches.

The **InvestigatoR** package leverages machine learning, particularly Keras-based models, to address these challenges. By offering flexible customization of loss functions, penalties, and activations, it enables users to design portfolios that maximize performance while adhering to practical constraints.

This vignette serves to:
1. Introduce the InvestigatoR package's capabilities for portfolio optimization.
2. Demonstrate its functionality through backtesting.
3. Validate its methods empirically, showcasing improvements in portfolio performance metrics like the Sharpe Ratio and Information Ratio.

---

# Setup

## Scientific Context and Requirements

The examples presented here are computationally intensive, as they involve training machine learning models and running backtests on financial data. To ensure reproducibility and efficiency:
- All results are cached to avoid re-running computations unnecessarily.
- The Python environment is configured for TensorFlow and Keras integration.

## Loading Libraries and Setting Up the Environment

```{r setup_libraries, include=TRUE}
# Load necessary libraries
library(reticulate)
library(tensorflow)
library(keras3)
library(dplyr)
library(tibble)
library(PerformanceAnalytics)

# Set up the Python virtual environment
reticulate::use_virtualenv("C:/R/python/")

# Confirm TensorFlow and Keras availability
reticulate::py_config()  
reticulate::py_module_available("tensorflow")
reticulate::py_module_available("keras")

# Load InvestigatoR package
devtools::load_all()
```

---

# Data Preparation

## Dataset Description

The dataset `data_ml` contains historical financial data, including features like dividend yields, market capitalization, and momentum. These features serve as inputs for training machine learning models to optimize portfolio weights.

The dataset spans multiple assets over a significant time horizon, providing a robust basis for backtesting.

```{r data_overview, echo=TRUE}
# Load the dataset
data("data_ml")

# Display dataset structure
str(data_ml)

# Summarize the dataset
data_ml %>%
  summarise(
    n_assets = n_distinct(stock_id),
    start_date = min(date),
    end_date = max(date)
  )
```

---

## Filtering the Data for Backtesting

To streamline computations, we will:
1. Restrict the dataset to the 100 assets with the most complete data.
2. Limit the time period to 2000-01-01 through 2015-12-31.

```{r filter_data, cache=TRUE}
# Identify assets with complete data for the full time period
stock_ids <- data_ml %>%
  group_by(stock_id) %>%
  summarise(
    start_date = min(date),
    end_date = max(date),
    n_obs = n()
  ) %>%
  filter(n_obs == max(n_obs)) %>% 
  distinct(stock_id) %>%
  dplyr::slice(1:100)  # Limit to 100 assets for speed

# Filter the dataset to a reduced subset
data_ml_red <- data_ml %>%
  right_join(stock_ids, by = "stock_id") %>%
  filter(date >= "2000-01-01", date <= "2015-12-31")
```

---

## Summarizing the Filtered Dataset

Before proceeding, we summarize the reduced dataset to verify it is ready for backtesting.

```{r data_summary, cache=TRUE}
# Summarize filtered dataset
data_ml_red %>%
  summarise(
    n_assets = n_distinct(stock_id),
    n_periods = n_distinct(date),
    start_date = min(date),
    end_date = max(date)
  )
```

---

This structured setup provides a solid foundation for the backtesting analysis that follows. In the next sections, we will:
- Configure machine learning models for portfolio optimization.
- Evaluate standalone and benchmark-tilted optimization techniques.
- Empirically validate their performance through metrics like the Sharpe Ratio and Information Ratio.

---

# Standalone Portfolio Optimization

Portfolio optimization without constraints often leads to unrealistic allocations, such as extreme leverage or high concentration in specific assets. In this section, we:
1. Configure a simple Keras model to optimize portfolio weights without constraints.
2. Backtest the portfolio and evaluate its performance.
3. Use `summary.weights` to highlight unreasonable weight distributions.
4. Apply postprocessing to enforce practical constraints and reassess performance.

---

## Unconstrained optimization

### Step 1: Define Portfolio Settings

To compare different optimization strategies, we define a set of portfolio constraints for backtesting:
- **Long-Only**: No short-selling; weights must be non-negative.
- **Weight Constraints**: Individual weights must not exceed 10%.
- **Leverage Limit**: Total exposure is capped at 1.0 (fully invested).
- **Diversification Penalty**: Avoid over-concentration in a few assets.

These constraints will be applied in later sections, but here we start with an unconstrained optimization.

---

### Step 2: Simple Keras Model (No Constraints)

#### Model Configuration

We start with a simple Keras model with:
- Three dense layers (32, 16, and 1 unit respectively).
- ReLU activation for hidden layers and no activation for the output layer.
- Loss function: `sharpe_ratio_loss` (no penalties applied).

```{r keras_simple_model, cache=TRUE}
# Define a simple Keras model configuration
config_keras_simple <- list(
  layers = list(
    list(type = "dense", units = 32, activation = "relu"),
    list(type = "dense", units = 16, activation = "relu"),
    list(type = "dense", units = 1)  # No custom activation
  ),
  loss = list(
    name = "sharpe_ratio_loss"  # Standard Sharpe Ratio loss with no penalties
  ),
  optimizer = list(name = "optimizer_adam", learning_rate = 0.001),
  epochs = 10,
  verbose = 0,
  seeds = c(42)  # Set random seed for reproducibility
)
```

---

### Step 3: Backtesting the Unconstrained Portfolio

We backtest the portfolio using the unconstrained model. This involves:
- Training the model on a rolling window of historical data.
- Evaluating performance using metrics like Sharpe Ratio and turnover.

```{r run_backtest_unconstrained, cache=TRUE}
# Define the portfolio configuration for the unconstrained model
pf_config_unconstrained <- list(
  keras_weights_simple = list(
    weight_func = "keras_weights",
    config1 = config_keras_simple
  )
)

# Run the backtest
portfolios_raw_unconstrained <- backtesting_weights(
  data = data_ml_red, 
  return_label = "R1M_Usd", 
  features = c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", "Ocf", "Pb", "Vol1Y_Usd"), 
  pf_config = pf_config_unconstrained,
  rolling = TRUE, 
  window_size = "5 years", 
  step_size = "1 year", 
  offset = "1 month", 
  in_sample = TRUE, 
  num_cores = 4, 
  verbose = TRUE
)
```

---

### Step 4: Analyzing Unconstrained Results

#### Weight Distributions

Unconstrained optimization often results in extreme weights. We use `summary.weights` to demonstrate this issue.

```{r summary_weights_unconstrained, cache=TRUE, results='asis'}
# Summarize portfolio weights
tt <- summary.weights(portfolios_raw_unconstrained, print=TRUE)
```

**Observation**: The weight summary reveals:
- Excessive leverage or underinvestment.
- Concentration in a few assets, leading to poor diversification.

---

#### Performance Metrics

We can evaluate the portfolio's performance using the `summary.performance` and/or `summary.performance2` functions. These provide insights into key metrics like Sharpe Ratio, turnover, and diversification.

```{r performance_unconstrained, cache=TRUE, results='asis'}
# Summarize portfolio performance
tt <- summary.performance2(portfolios_raw_unconstrained, test=TRUE, print=TRUE)
```

**Observation**: 
- `summary.performance`: Focuses on primary metrics like returns, volatility, and Sharpe Ratio.
- `summary.performance2`: Provides deeper insights into turnover, diversification, and other risk-adjusted metrics.

---

Would you like a more detailed explanation of the specific outputs of these functions, or should I continue enhancing the vignette?

**Observation**: While the Sharpe Ratio may appear high, other metrics like turnover and diversification are suboptimal, making the portfolio impractical for real-world use.

---

### Step 5: Postprocessing the Portfolio

To address these issues, we apply postprocessing to:
1. Normalize weights to enforce full investment.
2. Enforce constraints like weight caps and diversification penalties.

```{r postprocessing_unconstrained, cache=TRUE, results='asis'}
# Define postprocessing configuration
pp_config <- list(
  list(operation = "set_weightsum", sum = 1, allow_short_sale = FALSE)
)

# Apply postprocessing
portfolios_postprocessed <- postprocessing_portfolios(portfolios_raw_unconstrained, pp_config)

# Summarize postprocessed weights
tt <- summary.weights(portfolios_postprocessed, print=TRUE)

# Evaluate performance after postprocessing
tt <- summary.performance2(portfolios_postprocessed, test=TRUE, print=TRUE)

```

---

### Results and Insights

1. **Before Postprocessing**:
   - The unconstrained portfolio exhibited extreme weights, high turnover, and poor diversification.
   - While achieving a high Sharpe Ratio, the portfolio's impracticality limits its usefulness.

2. **After Postprocessing**:
   - Weight distributions improved significantly, adhering to realistic constraints.
   - Diversification and turnover metrics became more acceptable, making the portfolio viable for real-world implementation.

---

## Activation Functions: Enforcing Portfolio Constraints

Activation functions in the **InvestigatoR** package allow users to impose practical constraints on portfolio weights. This section demonstrates:
1. Long-only portfolios with `activation_box_sigmoid`.
2. Fully invested portfolios with `activation_box_sum_sigmoid`.
3. How combining multiple portfolios with `combine_portfolios` provides a comprehensive summary for analysis.

---

### Step 1: Long-Only Portfolio (Box Constraints)

The **`activation_box_sigmoid`** ensures:
- Non-negative weights (long-only).
- Bounded weights, e.g., within [0, 0.2].

```{r activation_box_sigmoid, cache=TRUE}
# Configuration with long-only activation
config_box_sigmoid <- list(
  layers = list(
    list(type = "dense", units = 32, activation = "relu"),
    list(type = "dense", units = 16, activation = "relu"),
    list(type = "dense", units = 1, activation = activation_box_sigmoid(min_weight = 0, max_weight = 0.2))
  ),
  loss = list(name = "sharpe_ratio_loss"),
  optimizer = list(name = "optimizer_adam", learning_rate = 0.001),
  epochs = 10,
  verbose = 0,
  seeds = c(42)
)

# Backtest with box-sigmoid activation
pf_config_box_sigmoid <- list(
  keras_weights_box_sigmoid = list(
    weight_func = "keras_weights",
    config1 = config_box_sigmoid
  )
)
portfolios_box_sigmoid <- backtesting_weights(
  data = data_ml_red, 
  return_label = "R1M_Usd", 
  features = c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", "Ocf", "Pb", "Vol1Y_Usd"), 
  pf_config = pf_config_box_sigmoid,
  rolling = TRUE, 
  window_size = "5 years", 
  step_size = "1 year", 
  offset = "1 month", 
  num_cores = 4, 
  verbose = TRUE
)
```

---

### Step 2: Fully Invested Portfolio (Sum Constraints)

The **`activation_box_sum_sigmoid`** normalizes weights to ensure they sum to a target value (e.g., 1 for fully invested portfolios).

```{r activation_box_sum_sigmoid, cache=TRUE}
# Configuration with sum constraints
config_box_sum_sigmoid <- list(
  layers = list(
    list(type = "dense", units = 32, activation = "relu"),
    list(type = "dense", units = 16, activation = "relu"),
    list(type = "dense", units = 1, activation = activation_box_sum_sigmoid(min_weight = 0, max_weight = 0.2, target_sum = 1))
  ),
  loss = list(name = "sharpe_ratio_loss"),
  optimizer = list(name = "optimizer_adam", learning_rate = 0.001),
  epochs = 10,
  verbose = 0,
  seeds = c(42)
)

# Backtest with box-sum-sigmoid activation
pf_config_box_sum_sigmoid <- list(
  keras_weights_box_sum_sigmoid = list(
    weight_func = "keras_weights",
    config1 = config_box_sum_sigmoid
  )
)
portfolios_box_sum_sigmoid <- backtesting_weights(
  data = data_ml_red, 
  return_label = "R1M_Usd", 
  features = c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", "Ocf", "Pb", "Vol1Y_Usd"), 
  pf_config = pf_config_box_sum_sigmoid,
  rolling = TRUE, 
  window_size = "5 years", 
  step_size = "1 year", 
  offset = "1 month", 
  num_cores = 4, 
  verbose = TRUE
)
```

---

### Step 3: Combining Portfolios for Comparative Analysis

To compare the results of the constrained cases, we use the **`combine_portfolios`** function to aggregate the portfolios.

```{r combine_portfolios, cache=TRUE}
# Combine portfolios for comparison
combined_portfolios <- combine_portfolioReturns(list(
  portfolios_raw_unconstrained,
  portfolios_postprocessed,
  portfolios_box_sigmoid,
  portfolios_box_sum_sigmoid
))

# Summarize combined weights
tt <- summary.weights(combined_portfolios, print=TRUE)

# Summarize combined performance
tt<- summary.performance2(combined_portfolios, test=TRUE, print=TRUE)
```

---

### Results and Insights

1. **Long-Only (Box Constraints)**:
   - Weights are constrained to non-negative values and limited to 20%.
   - Improved diversification compared to unconstrained portfolios.

2. **Fully Invested (Sum Constraints)**:
   - Weights are normalized to sum to 1, ensuring full investment.
   - Even better diversification and realistic allocation.

3. **Combined Results**:
   - The `combine_portfolios` function allows for easy side-by-side comparisons of weights and performance metrics.

These results highlight the importance of activation functions in creating portfolios that adhere to practical constraints while maintaining robust performance. In the next section, we introduce custom loss functions for further refinement of portfolio optimization.

---


