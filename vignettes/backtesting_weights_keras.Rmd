---
title: "backtesting_weights_keras"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{backtesting_weights_keras}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setuporig, echo=TRUE, include=FALSE}
#library(InvestigatoR)
# library(reticulate)
# library(tensorflow)
# library(keras3)
# library(dplyr)
# library(tibble)
# library(PerformanceAnalytics)
```
```{r setup, echo=FALSE, include=TRUE}
#library(InvestigatoR)
library(reticulate)
library(tensorflow)
library(keras3)
reticulate::use_virtualenv("../../python/")
# check for python availability and whether modules are installed
reticulate::py_config()  
reticulate::py_module_available("tensorflow")
reticulate::py_module_available("keras")
library(dplyr)
library(tibble)
library(PerformanceAnalytics)
devtools::load_all()
data("data_ml")
```

# Introduction

In this vignette, we demonstrate how to use the `InvestigatoR` package to directly optimize portfolio weights using Keras models. We progressively build up from a simple model to more advanced configurations that include custom activations, penalties on turnover, leverage, and diversification, and early stopping. Along the way, we explain the intuition behind each step, and finally, we'll backtest the models and analyze the results.

---

## Step 1: A Simple Keras Model

We start with a basic 3-layer neural network using ReLU activations and no custom constraints or penalties. This step introduces the basics of setting up a Keras model to predict portfolio weights.

```{r keras_simple_model}
# Simple 3-layer Keras model, no callbacks or custom activations
config_keras_simple <- list(
  layers = list(
    list(type = "dense", units = 32, activation = "relu"),
    list(type = "dense", units = 16, activation = "relu"),
    list(type = "dense", units = 1)  # No custom activation
  ),
  loss = list(
    name = "sharpe_ratio_loss", 
    transaction_costs = 0.001, 
    delta = 0.1, 
    lambda = 0.1, 
    leverage = 1.0, 
    eta = 0.1  # Standard Sharpe ratio loss with penalties
  ),
  optimizer = list(name = "optimizer_rmsprop", learning_rate = 0.001),
  epochs = 10,
  verbose = 0,
  seeds = c(42), # 123, 456, 17, 55), # set random seeds to average over results
  plot_training = FALSE,
  plot_result = FALSE
)
```

### Explanation

This simple model will help us understand how a neural network allocates portfolio weights based purely on the input data, without any additional constraints or penalties.

### Running the first Backtest

```{r portfolio_config}
# Define portfolio config with all Keras models
pf_config1 <- list(
  keras_weights_simple = list(
    weight_func = "keras_weights",
    config1 = config_keras_simple
  )
)
```

We now run the backtest using the Keras model we defined above and evaluate their performance based on Sharpe ratios, turnover, diversification, and leverage.

```{r run_backtest}
# Run the backtest with #1st Keras configurations
keras_portfolios <- backtesting_weights(
  data = data_ml, 
  return_label = "R1M_Usd", 
  features = c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", "Ocf", "Pb", "Vol1Y_Usd"), 
  pf_config = pf_config1,  # Portfolio config with multiple Keras configurations
  rolling = TRUE, 
  window_size = "5 years", 
  step_size = "1 year", 
  offset = "1 month", 
  in_sample = TRUE, 
  num_cores = 4, 
  verbose = TRUE
)
```

---

## Step 2: Add Custom Activations (Long-Only Portfolio)

In this step, we introduce a custom activation function (`activation_box_sigmoid`) that constrains the portfolio to long-only positions, ensuring that all portfolio weights are positive and bounded between 0 and 0.2.

```{r keras_with_activation}
# Keras model with custom activation (long-only constraint)
config_keras_with_activation <- list(
  layers = list(
    list(type = "dense", units = 32, activation = "relu"),
    list(type = "dense", units = 16, activation = "relu"),
    list(type = "dense", units = 1, activation = activation_box_sigmoid(min_weight = 0, max_weight = 0.2))  # Long-only
  ),
  loss = list(name = "sharpe_ratio_loss"),
  optimizer = list(name = "optimizer_rmsprop", learning_rate = 0.001),
  epochs = 10,
  verbose = 0,
  seeds = c(42), # 123, 456, 17, 55), # set random seeds to average over results
  plot_training = FALSE,
  plot_result = FALSE
)
```

### Why Custom Activations?

This step introduces long-only constraints, ensuring that all portfolio weights are non-negative. This is essential for many portfolios that disallow short selling.

---

## Step 3: Add Early Stopping

We now increase the number of epochs and introduce early stopping. Early stopping helps prevent overfitting by monitoring the loss and halting training when no improvement is detected.

```{r keras_with_early_stopping}
# Keras model with early stopping
config_keras_with_early_stopping <- config_keras_with_activation
config_keras_with_early_stopping$callbacks <- list(
  callback_early_stopping(monitor = "loss", min_delta = 0.001, patience = 3)
  )
```

### Why Early Stopping?

Increasing the number of epochs gives the model more time to learn, but early stopping ensures that training halts once the loss stops improving, reducing overfitting.

---

## Step 4: Increase Diversification Penalty

In this step, we increase the diversification penalty (`lambda`). This encourages the model to diversify the portfolio, reducing over-concentration in a few assets.

```{r keras_increase_diversification}
# Keras model with a higher diversification penalty
config_keras_increase_diversification <- config_keras_with_early_stopping
config_keras_increase_diversification$loss$lambda <- 0.7  # Stronger diversification penalty

config_keras_increase_diversification <- list(
  layers = list(
    list(type = "dense", units = 32, activation = "relu"),
    list(type = "dense", units = 16, activation = "relu"),
    list(type = "dense", units = 1, activation = activation_box_sigmoid(min_weight = 0, max_weight = 0.2))
  ),
  loss = list(
    name = "sharpe_ratio_loss", 
    transaction_costs = 0.001, 
    delta = 0.1, 
    lambda = 0.7,  # Stronger diversification penalty
    leverage = 1.0, 
    eta = 0.1
  ),
  optimizer = list(name = "optimizer_adam", learning_rate = 0.001),
  metrics = list(diversification_metric()),
  callbacks = list(
    callback_early_stopping(monitor = "loss", min_delta = 0.001, patience = 3)
  ),
  epochs = 100,  # More epochs with early stopping
  verbose = 0,
  seeds = c(42), # 123, 456, 17, 55), # set random seeds to average over results
  plot_training = FALSE,
  plot_result = FALSE
)
```

### Why Increase Diversification Penalty?

Increasing the diversification penalty encourages the model to spread its portfolio across a wider range of assets, reducing the risk of over-concentration.

---

## Step 5: Increase Leverage Penalty

We now increase the leverage penalty (`eta`), which discourages the model from using high leverage, limiting the portfolio's exposure to excessive risk.

```{r keras_increase_leverage}
# Keras model with a higher leverage penalty
config_keras_increase_leverage <- config_keras_with_early_stopping
config_keras_increase_leverage$loss$eta <- 0.7  # Stronger leverage penalty
```

### Why Increase Leverage Penalty?

A higher leverage penalty reduces the likelihood of the model allocating too much weight to individual assets, mitigating the risk of excessive leverage.

---

## Step 6: Add Turnover Penalty

Turnover refers to the frequency with which assets are bought and sold in a portfolio. High turnover can incur additional costs. We add a penalty to discourage high turnover.

```{r keras_increase_turnover}
# Keras model with turnover penalty
config_keras_increase_turnover <- config_keras_with_early_stopping
config_keras_increase_turnover$loss$transaction_costs <- 0.01  # Increase transaction costs to penalize high turnover
```

### Why Add a Turnover Penalty?

Controlling turnover is important because frequent trading can lead to higher transaction costs, which can erode returns over time. Adding a penalty helps balance between achieving returns and controlling costs.

---

## Portfolio Configuration for Backtesting

Now that we've defined our models, we set up the portfolio configuration to test all of these different Keras setups in one backtest.

```{r portfolio_config2}
# Define portfolio config with all Keras models
pf_config <- list(
  keras_weights_simple = list(
    weight_func = "keras_weights",
    #config1 = config_keras_simple,
    config2 = config_keras_with_activation,
    config3 = config_keras_with_early_stopping,
    config4 = config_keras_increase_diversification,
    config5 = config_keras_increase_leverage,
    config6 = config_keras_increase_turnover
  )
)
```

---

## Running the Backtest

We now run the backtest using the different Keras models we defined above. We will evaluate their performance based on Sharpe ratios, turnover, diversification, and leverage.

```{r run_backtest2}
# Run the backtest with all Keras configurations
data("data_ml")
keras_portfolios <- backtesting_weights(
  data = data_ml, 
  portfolio_object = keras_portfolios,
  return_label = "R1M_Usd", 
  features = c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", "Ocf", "Pb", "Vol1Y_Usd"), 
  pf_config = pf_config,  # Portfolio config with multiple Keras configurations
  rolling = TRUE, 
  window_size = "5 years", 
  step_size = "1 year", 
  offset = "1 month", 
  in_sample = TRUE, 
  num_cores = 5, 
  verbose = TRUE
)
```

---


## Analyzing Results

We analyze the results of the backtest to see how the different configurations performed in terms of turnover, leverage, diversification, and Sharpe ratio.

```{r analyze_results}
pp_config <- list(
  list(
    operation = "set_weightsum",
    sum = 1,
    allow_short_sale = FALSE
  ),
  list(
    operation = "flatten_weights",
    l1 = 0.01,
    l2 = 0.01,
    mix = 0.7
  ),
  list(
    operation = "reduce_turnover",
    method = "exponential",
    smoothing_factor = 0.2
  ),
  list(
    operation = "increase_diversification",
    hh_target = 0.3
  )
)

keras_portfolios_pp <- postprocessing_portfolios(keras_portfolios, pp_config)
summary(keras_portfolios)
summary(keras_portfolios_pp)

# Visualize the portfolio performance
plot(keras_portfolios)
plot(keras_portfolios_pp)

# Additional analysis using PerformanceAnalytics
summary(keras_portfolios, type = "table.AnnualizedReturns")
summary(keras_portfolios, type = "table.AnnualizedReturns")
summary(keras_portfolios_pp, type = "table.Distributions")
summary(keras_portfolios_pp, type = "table.Distributions")
```


## Utilize Advanced Loss Functions

In this step, we incorporate advanced loss functions (`sharpe_ratio_difference_loss`, `information_ratio_loss_active_returns`, and `information_ratio_loss_regression_based`) into our Keras models. Additionally, we compute benchmark weights based on the `Mkt_Cap_3M_Usd` column to create a value-weighted benchmark and adjust the activation function to bound the weight changes (`delta_w`) between -0.2 and 0.2.

### Define Benchmark Weights Based on Market Capitalization

We compute the benchmark weights for each date by using the `Mkt_Cap_3M_Usd` column. This ensures that the benchmark is value-weighted.

```{r compute_benchmark_weights, echo=TRUE}
# Create value-weighted benchmark weights based on Mkt_Cap_3M_Usd
data_ml <- data_ml %>%
  group_by(date) %>%
  mutate(benchmark = Mkt_Cap_3M_Usd / sum(Mkt_Cap_3M_Usd)) %>%
  ungroup()
```


### Configure Models with Advanced Loss Functions

We create separate configurations for each advanced loss function, incorporating the custom activation function and the value-weighted benchmark.

#### Sharpe Ratio Difference Loss

```{r config_sharpe_ratio_difference, echo=TRUE}
# Configuration for Sharpe Ratio Difference Loss
config_sharpe_ratio_difference <- list(
  layers = list(
    list(type = "dense", units = 32, activation = "relu"),
    list(type = "dense", units = 16, activation = "relu"),
    list(type = "dense", units = 1, activation = activation_box_sigmoid(min_weight = -0.2, max_weight = 0.2))  # Bound delta_w
  ),
  loss = list(
    name = "sharpe_ratio_difference_loss",
    lambda_l1 = 0.01,
    lambda_l2 = 0.01
  ),
  optimizer = list(name = "optimizer_adam", learning_rate = 0.001),
  metrics = list(distance_from_benchmark_l1_metric(), distance_from_benchmark_l2_metric()),
  callbacks = list(
    callback_early_stopping(monitor = "loss", min_delta = 0.001, patience = 3)
  ),
  epochs = 50,
  batch_size = 128,
  verbose = 0,
  seeds = c(42),
  plot_training = FALSE,
  plot_result = FALSE
)
```

#### Information Ratio Loss (Active Returns)

```{r config_information_ratio_active, echo=TRUE}
# Configuration for Information Ratio Loss based on Active Returns
config_information_ratio_active <- config_sharpe_ratio_difference
config_information_ratio_active$loss$name <- "information_ratio_loss_active_returns"
```

#### Information Ratio Loss (Regression-Based)

```{r config_information_ratio_regression, echo=TRUE}
# Configuration for Information Ratio Loss based on Regression
config_information_ratio_regression <- config_sharpe_ratio_difference
config_information_ratio_regression$loss$name <- "information_ratio_loss_regression_based"
```

### Define Portfolio Configurations

We aggregate all configurations into a portfolio configuration list to be used in the backtest.

```{r define_portfolio_config, echo=TRUE}
# Define portfolio config with all advanced Keras configurations
pf_config_advanced <- list(
  keras_weights_sharpe_ratio_difference = list(
    weight_func = "keras_weights",
    config1 = config_sharpe_ratio_difference,
    config2 = config_information_ratio_active,
    config3 = config_information_ratio_regression
  )
)
```

### Run Backtests with Advanced Configurations

We execute the backtests for each advanced configuration and evaluate their performance.

```{r run_advanced_backtests, echo=TRUE}
# Run the backtest with advanced Keras configurations
advanced_keras_portfolios <- backtesting_weights(
  data = data_ml, 
  return_label = "R1M_Usd", 
  benchmark_label = "benchmark",
  portfolio_object = keras_portfolios,
  features = features, 
  pf_config = pf_config_advanced,  # Portfolio config with advanced Keras configurations
  rolling = TRUE, 
  window_size = "5 years", 
  step_size = "1 year", 
  offset = "1 month", 
  in_sample = TRUE, 
  num_cores = 20, 
  verbose = TRUE
)
```

### Analyze the Results

We summarize and visualize the performance of each advanced Keras model.

```{r analyze_advanced_results, echo=TRUE}
advanced_keras_portfolios_pp <- postprocessing_portfolios(advanced_keras_portfolios, pp_config)
summary(advanced_keras_portfolios)
summary(advanced_keras_portfolios_pp)

# Visualize the portfolio performance
plot(advanced_keras_portfolios_pp)

# Additional analysis using PerformanceAnalytics
summary(advanced_keras_portfolios_pp, type = "table.AnnualizedReturns")
summary(advanced_keras_portfolios_pp, type = "table.Distributions")
# Summarize the portfolio results
```

### Interpretation of Results

Each advanced loss function brings a unique perspective to portfolio optimization:

- **Sharpe Ratio Difference Loss**: Focuses on maximizing the difference in Sharpe ratios between the portfolio and the benchmark.
- **Information Ratio Loss (Active Returns)**: Aims to maximize the Information Ratio based on active returns relative to the benchmark.
- **Information Ratio Loss (Regression-Based)**: Enhances the Information Ratio by considering regression-based alpha and idiosyncratic return variability.

By bounding `delta_w` between -0.2 and 0.2, we ensure that the portfolio adjustments remain moderate, avoiding extreme weight shifts that could lead to high turnover or leverage.

---

## Conclusion

In this extended vignette, we've demonstrated how to leverage advanced custom loss functions within the `InvestigatoR` package to optimize portfolio weights using Keras models. By computing value-weighted benchmarks and constraining weight adjustments, we enhance the robustness and interpretability of our portfolio optimization strategies. The incorporation of penalties for turnover, leverage, and diversification further refines the portfolio's performance, balancing return objectives with risk management.
